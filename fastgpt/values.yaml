# Default values for fastgpt.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  #repository: "ghcr.io/labring/fastgpt"
  repository: "liuxu/fastgpt-debug"
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v20240515"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 3000

ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 500m
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: fastgpt-debug.linux.plus
      paths:
        - path: /
          pathType: Prefix
  tls:
  - secretName: linux-plus-tls
    hosts:
      - "*.linux.plus"

resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  limits:
    cpu: 4000m
    memory: 8000Mi
  requests:
    cpu: 2000m
    memory: 4000Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes:
- name: fastgpt-env

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

dotenv:
  enabled: true
  data:
    DEFAULT_ROOT_PSW: "hkzn123"
    # 中转地址，如果是用官方号，不需要管。务必加 /v1
    OPENAI_BASE_URL: "https://www.gptapi.us/v1"
    CHAT_API_KEY: "sk-badLcLwDlZig7sCk8968C2Bd1cA34951Be8688B337EeC528"
    #OPENAI_BASE_URL: "https://api.dwyu.top/v1"
    #CHAT_API_KEY: "sk-Jfds42uk6FfcxOpLD74f8430EeEf447bBcA16dBcAb6cC478"
    #OPENAI_BASE_URL=https://api.openai-ch.top/v1
    #CHAT_API_KEY=sk-hpZPJzSpjnsX9xqK25D2Ea18310c47C5B52aC99303373fEf

    DB_MAX_LINK: 100
    TOKEN_KEY: "any"
    ROOT_KEY: "root_key"
    FILE_TOKEN_KEY: "filetoken"
    MONGODB_URI: "mongodb://myusername:mypassword@fastgpt-mongodb:27017/fastgpt?authSource=admin"
    PG_URL: "postgresql://username:password@fastgpt-pg:5432/postgres"

configJson:
  {
    "systemEnv": {
      "openapiPrefix": "fastgpt",
      "vectorMaxProcess": 15,
      "qaMaxProcess": 15,
      "pgHNSWEfSearch": 100
    },
    "llmModels": [
      {
        "model": "gpt-3.5-turbo",
        "name": "gpt-3.5-turbo",
        "maxContext": 16000,
        "maxResponse": 4000,
        "quoteMaxToken": 13000,
        "maxTemperature": 1.2,
        "charsPointsPrice": 0,
        "censor": false,
        "vision": false,
        "datasetProcess": false,
        "usedInClassify": true,
        "usedInExtractFields": true,
        "useInToolCall": true,
        "usedInQueryExtension": true,
        "toolChoice": true,
        "functionCall": false,
        "customCQPrompt": "",
        "customExtractPrompt": "",
        "defaultSystemChatPrompt": "",
        "defaultConfig": {}
      },
      {
        "model": "gpt-3.5-turbo-16k",
        "name": "gpt-3.5-turbo-16k",
        "maxContext": 16000,
        "maxResponse": 16000,
        "quoteMaxToken": 13000,
        "maxTemperature": 1.2,
        "charsPointsPrice": 0,
        "censor": false,
        "vision": false,
        "datasetProcess": true,
        "usedInClassify": true,
        "usedInExtractFields": true,
        "useInToolCall": true,
        "usedInQueryExtension": true,
        "toolChoice": true,
        "functionCall": false,
        "customCQPrompt": "",
        "customExtractPrompt": "",
        "defaultSystemChatPrompt": "",
        "defaultConfig": {}
      },
      {
        "model": "gpt-4-0125-preview",
        "name": "gpt-4-turbo",
        "maxContext": 125000,
        "maxResponse": 4000,
        "quoteMaxToken": 100000,
        "maxTemperature": 1.2,
        "charsPointsPrice": 0,
        "censor": false,
        "vision": false,
        "datasetProcess": false,
        "usedInClassify": true,
        "usedInExtractFields": true,
        "useInToolCall": true,
        "usedInQueryExtension": true,
        "toolChoice": true,
        "functionCall": false,
        "customCQPrompt": "",
        "customExtractPrompt": "",
        "defaultSystemChatPrompt": "",
        "defaultConfig": {}
      },
      {
        "model": "gpt-4-vision-preview",
        "name": "gpt-4-vision",
        "maxContext": 128000,
        "maxResponse": 4000,
        "quoteMaxToken": 100000,
        "maxTemperature": 1.2,
        "charsPointsPrice": 0,
        "censor": false,
        "vision": true,
        "datasetProcess": false,
        "usedInClassify": false,
        "usedInExtractFields": false,
        "useInToolCall": false,
        "usedInQueryExtension": false,
        "toolChoice": true,
        "functionCall": false,
        "customCQPrompt": "",
        "customExtractPrompt": "",
        "defaultSystemChatPrompt": "",
        "defaultConfig": {}
      }
    ],
    "vectorModels": [
      {
        "model": "text-embedding-ada-002",
        "name": "Embedding-2",
        "charsPointsPrice": 0,
        "defaultToken": 700,
        "maxToken": 3000,
        "weight": 100
      },
      {
        "model": "text-embedding-3-small",
        "name": "Embedding-3-small",
        "charsPointsPrice": 0,
        "defaultToken": 700,
        "maxToken": 3000,
        "weight": 100
      },
      {
        "model": "text-embedding-3-large",
        "name": "Embedding-3-large",
        "charsPointsPrice": 0,
        "defaultToken": 700,
        "maxToken": 3000,
        "weight": 100
      }
    ],
    "reRankModels": [],
    "audioSpeechModels": [
      {
        "model": "tts-1",
        "name": "OpenAI TTS1",
        "charsPointsPrice": 0,
        "voices": [
          {
            "label": "Alloy",
            "value": "alloy",
            "bufferId": "openai-Alloy"
          },
          {
            "label": "Echo",
            "value": "echo",
            "bufferId": "openai-Echo"
          },
          {
            "label": "Fable",
            "value": "fable",
            "bufferId": "openai-Fable"
          },
          {
            "label": "Onyx",
            "value": "onyx",
            "bufferId": "openai-Onyx"
          },
          {
            "label": "Nova",
            "value": "nova",
            "bufferId": "openai-Nova"
          },
          {
            "label": "Shimmer",
            "value": "shimmer",
            "bufferId": "openai-Shimmer"
          }
        ]
      }
    ],
    "whisperModel": {
      "model": "whisper-1",
      "name": "Whisper1",
      "charsPointsPrice": 0
    }
  }

nodeSelector: {}

tolerations: []

affinity: {}
